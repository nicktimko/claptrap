{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import struct\n",
    "\n",
    "import networkx as nx\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELEMENT_MATCHER = r'((Dr|Mr|Mrs|Prof).|\\b[a-zA-Z\\']+\\b|;|:|,|\\.|\\?|!)'  # whole words and some punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_TRANS = str.maketrans({\n",
    "    '\\N{LEFT SINGLE QUOTATION MARK}': \"'\",\n",
    "    '\\N{RIGHT SINGLE QUOTATION MARK}': \"'\",\n",
    "    '\\N{LEFT DOUBLE QUOTATION MARK}': '\"',\n",
    "    '\\N{RIGHT DOUBLE QUOTATION MARK}': '\"',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(*, filename=None, text=None, harmonize_caps=True, trans=None):\n",
    "    if filename:\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            lines = [line for line in f if not line.isupper()]\n",
    "    else:\n",
    "        lines = text.splitlines()\n",
    "\n",
    "    if trans is None:\n",
    "        trans = QUOTE_TRANS\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.translate(trans)\n",
    "        yield from line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genwords2(*args, **kwargs):\n",
    "    # initial cleaning/fragmenting\n",
    "    pass1 = words(*args, **kwargs)\n",
    "    \n",
    "    # find most common capitalizations\n",
    "    most_common_capitalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(*, filename=None, text=None, harmonize_caps=True):\n",
    "    if filename:\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            lines = [line for line in f if not line.isupper()]\n",
    "    else:\n",
    "        lines = text.splitlines()\n",
    "        \n",
    "    if harmonize_caps:\n",
    "        translator = most_common_capitalization(gen_words(filename=filename, text=text, harmonize_caps=False))\n",
    "    else:\n",
    "        translator = {}\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.translate(QUOTE_TRANS)\n",
    "        for word in re.findall(ELEMENT_MATCHER, line):\n",
    "            word = word[0]\n",
    "            yield translator.get(word, word)\n",
    "            \n",
    "\n",
    "def most_common_capitalization(words):\n",
    "    case_sensitive = collections.Counter(words)\n",
    "    case_insensitive = collections.defaultdict(dict)\n",
    "    for word, count in case_sensitive.items():\n",
    "        case_insensitive[word.lower()][word] = count\n",
    "        \n",
    "    translator = {}    \n",
    "    for lower, counts in case_insensitive.items():\n",
    "        if len(counts) < 2:\n",
    "            continue\n",
    "        winner = max(counts.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "        for variant in counts:\n",
    "            translator[variant] = winner\n",
    "\n",
    "    return translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK DRACULA ***'\n",
    "end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK DRACULA ***'\n",
    "\n",
    "dracula = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt').text\n",
    "dracula = dracula[dracula.find(start_marker) + len(start_marker):dracula.find(end_marker)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "drac_corpus = list(gen_words(text=dracula))[231:]\n",
    "drac_counter = collections.Counter(drac_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11243 ,       7999 .       7907 the     5905 and     4800 I       4663 to      3629 of      \n",
      "2954 a       2567 he      2507 in      2469 that    2156 it      1879 was     1684 ;       \n",
      "1590 as      1543 we      1537 for     1501 is      1471 his     1455 me      1402 not     \n",
      "1397 you     1286 with    1261 my      1164 all     1115 be      1107 so      1083 at      \n",
      "1073 on      1071 but     1060 have    1059 her     1039 had      953 him      813 she     \n",
      " 776 when     773 there    751 !        663 which    638 if       636 :        631 this    \n",
      " 623 from     582 are      570 said     552 were     549 then     518 by       496 one     \n",
      " 492 ?        491 could    488 no       474 do       472 them     463 or       463 they    \n",
      " 463 what     462 us       454 will     448 up       442 some     440 must     432 would   \n",
      " 430 out      426 shall    414 may      407 our      407 now      395 see      391 know    \n",
      " 391 been     390 time     377 can      376 more     359 an       345 has      339 come    \n",
      " 336 over     336 am       322 Van      310 night    309 came     303 your     300 Helsing \n",
      " 299 went     292 like     288 into     287 only     285 any      284 go       284 who     \n",
      " 283 did      279 very     277 before   271 here     260 back     258 good     253 down    \n",
      " 246 again    245 well     243 seemed   242 even     232 about    231 way      231 room    \n",
      " 227 such     227 man      224 took     223 dear     221 though   221 Lucy     220 how     \n",
      " 220 day      220 much     219 than     219 saw      219 their    218 think    216 where   \n",
      " 215 too      214 through  210 after    210 hand     204 Mina     204 face     200 door    \n",
      " 198 should   195 tell     194 made     193 poor     192 sleep    190 old      189 away    \n",
      " 188 eyes     187 own      186 looked   183 great    183 friend   182 once     175 things  \n",
      " 174 Jonathan 174 other    172 get      171 look     171 Profe    169 just     168 little  \n",
      " 167 Dr.      162 make     158 might    157 got      156 yet      155 off      154 thought \n",
      " 153 found    153 Count    150 take     150 God      148 long     148 let      148 say     \n",
      " 148 life     146 work     146 men      144 told     143 asked    142 something 142 Oh      \n",
      " 140 last     139 heart    137 place    137 fear     136 ever     135 without  135 till    \n",
      " 135 first    135 its      135 house    134 myself   134 Arthur   133 two      128 done    \n",
      " 128 knew     128 never    122 himself  117 still    117 coming   116 window   116 want    \n",
      " 115 find     115 these    115 quite    113 began    113 Harker   113 blood    112 nothing \n",
      " 110 same     109 dead     108 round    107 morning "
     ]
    }
   ],
   "source": [
    "per_line = 7\n",
    "for n, (block, count) in enumerate(drac_counter.most_common(200)):\n",
    "    print('{:>4d} {:<8s}'.format(count, block), end='')\n",
    "    if n % per_line == per_line - 1:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwords = iter(drac_corpus)\n",
    "prev = next(iwords)\n",
    "G = nx.DiGraph()\n",
    "for word in iwords:\n",
    "    if G.has_edge(prev, word):\n",
    "        G[prev][word]['weight'] += 1\n",
    "    else:\n",
    "        G.add_edge(prev, word, weight=1)\n",
    "    prev = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3648988"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nld = nx.readwrite.json_graph.node_link_data(G)\n",
    "# nld.pop('directed'), nld.pop('multigraph')\n",
    "len(json.dumps(nld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2157962\n",
      "True False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adjacency': 9334, 'graph': 0, 'nodes': 9334}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjd = nx.readwrite.json_graph.adjacency_data(G)\n",
    "print(len(json.dumps(adjd)))\n",
    "\n",
    "print(adjd.pop('directed'), adjd.pop('multigraph'))\n",
    "{k: len(adjd[k]) for k in adjd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'abcd',)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct.unpack_from('4s', b'abcd\\0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x200000'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(unpack_varint([129, 128, 128, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_varuint(n):\n",
    "    if n < 0:\n",
    "        raise ValueError('value must be non-negative')\n",
    "    packed = [n & 0x7F]\n",
    "    n >>= 7\n",
    "    while n > 0:\n",
    "        packed.insert(0, 0x80 | (n & 0x7F))\n",
    "        n >>= 7\n",
    "\n",
    "    return packed\n",
    "\n",
    "def unpack_varuint(byteiter):\n",
    "    n = 0\n",
    "    for byte in byteiter:\n",
    "        n <<= 7\n",
    "        n += byte & 0x7F\n",
    "        if not (byte & 0x80):\n",
    "            return n\n",
    "    raise ValueError('byte iterator exhausted')\n",
    "        \n",
    "def test_unpack_varuint():\n",
    "    assert unpack_varuint(b'\\x00') == 0\n",
    "    assert unpack_varuint(b'\\x00\\x00') == 0\n",
    "    assert (unpack_varuint(b'\\x81\\x00') ==\n",
    "            unpack_varuint(b'\\x81\\x00\\x00') ==\n",
    "            128)\n",
    "    \n",
    "    bi = iter(b'\\x00\\x00')\n",
    "    assert unpack_varuint(bi) == 0\n",
    "    assert unpack_varuint(bi) == 0\n",
    "\n",
    "def test_packunpack_varuint():\n",
    "    big = 1234567890212345678902345432\n",
    "    nums = itertools.chain(\n",
    "        range(10000), \n",
    "        range(0x3FF0, 0x400F), \n",
    "        range(0x1FFFF0, 0x20000F),\n",
    "        range(big, big + 1234),\n",
    "    )\n",
    "    for n in nums:\n",
    "        packed = pack_varuint(n)\n",
    "        assert n == unpack_varuint(packed)\n",
    "        assert not (packed[-1] & 0x80)\n",
    "        assert all(px & 0x80 for px in packed[:-1])\n",
    "        \n",
    "test_packunpack_varuint()\n",
    "test_unpack_varuint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'3215'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b'321' + bytes([53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pack_str(s):\n",
    "    b = s.encode('utf-8')\n",
    "    return bytes([len(b)]) + bytes(b)\n",
    "\n",
    "def unpack_str(byteiter):\n",
    "    byteiter = iter(byteiter)\n",
    "    n = next(byteiter)\n",
    "    b = bytes(itertools.islice(byteiter, n))\n",
    "    return b.decode('utf-8')\n",
    "\n",
    "def test_packunpack_str():\n",
    "    for word in itertools.__doc__.split():\n",
    "        unpack_str(pack_str(word)) == word\n",
    "        \n",
    "test_packunpack_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump(type_):\n",
    "    '''\n",
    "    Feed the generator into *type_* til exhaustion. Can be overridden by\n",
    "    setting exhaust=False.\n",
    "    '''\n",
    "    def _pump(f):\n",
    "        def fx(*args, **kwargs):\n",
    "            exhaust_gen = kwargs.pop('exhaust', True)\n",
    "            gen = f(*args, **kwargs)\n",
    "            if exhaust_gen:\n",
    "                return type_(gen)\n",
    "            return gen\n",
    "        return fx\n",
    "    return _pump\n",
    "\n",
    "@pump(bytes)\n",
    "def pack_strarr(strs):\n",
    "    yield from pack_varuint(len(strs))\n",
    "    for s in strs:\n",
    "        yield from pack_str(s)\n",
    "    \n",
    "@pump(list)\n",
    "def unpack_strarr(byteiter):\n",
    "    byteiter = iter(byteiter)\n",
    "    n = unpack_varuint(byteiter)\n",
    "    for _ in range(n):\n",
    "        yield unpack_str(byteiter)\n",
    "    \n",
    "def test_pack_strarr():\n",
    "    assert pack_strarr([]) == b'\\x00'\n",
    "    assert pack_strarr(['hey', 'there', '\\N{UNICORN FACE}']) == b'\\x03\\x03hey\\x05there\\x04\\xf0\\x9f\\xa6\\x84'\n",
    "    \n",
    "def test_unpack_strarr():\n",
    "    assert unpack_strarr(b'\\x00') == []\n",
    "    assert unpack_strarr(b'\\x01\\x01x') == ['x']\n",
    "    assert unpack_strarr(b'\\x01\\x01x' + bytes(100)) == ['x']\n",
    "    \n",
    "def test_packunpack_strarr():\n",
    "    def pup(x):\n",
    "        return unpack_strarr(pack_strarr(x))\n",
    "    a1 = ['x'] * 10000\n",
    "    assert pup(a1) == a1\n",
    "    \n",
    "    x = ord('\\N{SLIGHTLY SMILING FACE}')\n",
    "    emojis = [chr(n) for n in range(x, x + 130)]\n",
    "    assert pup(emojis) == emojis\n",
    "    \n",
    "test_pack_strarr()\n",
    "test_unpack_strarr()\n",
    "test_packunpack_strarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pump(bytes)\n",
    "def pack_intarr_2d(arr):\n",
    "    rows = len(arr)\n",
    "    if rows == 0:\n",
    "        yield from b'\\x00\\x00'\n",
    "        return\n",
    "    yield from pack_varuint(rows)\n",
    "    cols = len(arr[0])\n",
    "    yield from pack_varuint(cols)\n",
    "    for row in arr:\n",
    "        if len(row) != cols:\n",
    "            raise ValueError('jagged array')\n",
    "        for element in row:\n",
    "            yield from pack_varuint(element)\n",
    "\n",
    "def test_pack_intarr():\n",
    "    assert pack_intarr_2d([]) == b'\\x00\\x00'\n",
    "    assert pack_intarr_2d([[]]) == b'\\x01\\x00'\n",
    "    assert pack_intarr_2d([[], []]) == b'\\x02\\x00'\n",
    "    assert pack_intarr_2d([[1]]) == bytes([1] * 3)\n",
    "    assert pack_intarr_2d([[2, 2], [2, 2]]) == bytes([2] * 6)\n",
    "    assert pack_intarr_2d([[3] * 3] * 3) == bytes([3] * 11)\n",
    "\n",
    "def test_pack_intarr_bad():\n",
    "    try:\n",
    "        pack_intarr_2d([1, 2])\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    else:\n",
    "        assert False, 'allowed bad array'\n",
    "        \n",
    "    try:\n",
    "        pack_intarr_2d([[1], [2, 2]])\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    else:\n",
    "        assert False, 'allowed bad array'\n",
    "    \n",
    "test_pack_intarr()\n",
    "test_pack_intarr_bad()\n",
    "\n",
    "@pump(list)\n",
    "def unpack_intarr_2d(byteiter):\n",
    "    byteiter = iter(byteiter)\n",
    "    rows = unpack_varuint(byteiter)\n",
    "    cols = unpack_varuint(byteiter)\n",
    "    for row in range(rows):\n",
    "        yield [unpack_varuint(byteiter) for _ in range(cols)]\n",
    "        \n",
    "def test_unpack_intarr():\n",
    "    assert unpack_intarr_2d(b'\\x00\\x00') == []\n",
    "    assert unpack_intarr_2d(b'\\x01\\x00') == [[]]\n",
    "    assert unpack_intarr_2d(b'\\x02\\x00') == [[], []]\n",
    "    \n",
    "    assert unpack_intarr_2d(b'\\x01\\x01\\x0F') == [[15]]\n",
    "    assert unpack_intarr_2d(bytes([2, 2, 1, 2, 3, 4])) == [[1, 2], [3, 4]]\n",
    "    assert unpack_intarr_2d(bytes([1, 2, 3, 4, 5, 6])) == [[3, 4]]\n",
    "    \n",
    "    byteiter = iter(bytes([1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]))\n",
    "    assert unpack_intarr_2d(byteiter) == [[3, 1]]\n",
    "    assert unpack_intarr_2d(byteiter) == [[1, 2, 3], [1, 2, 3]]\n",
    "    assert unpack_intarr_2d(byteiter) == [[3, 1]]\n",
    "    \n",
    "test_unpack_intarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unpack_strarr(b'\\x01\\x02hi')) == ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_digraph(G):\n",
    "    adjd = nx.readwrite.json_graph.adjacency_data(G)\n",
    "    return serialize_adjd(adjd)\n",
    "\n",
    "@pump(bytes)\n",
    "def serialize_adjd(adjd):\n",
    "    if not adjd['directed'] or adjd['multigraph']:\n",
    "        raise ValueError('only supports directed simple graphs')\n",
    "        \n",
    "    nodes = [n['id'] for n in adjd['nodes']]\n",
    "    lookup = {nodeword: n for n, nodeword in enumerate(nodes)}\n",
    "    \n",
    "    yield from pack_strarr(nodes)\n",
    "\n",
    "    for adj in adjd['adjacency']:\n",
    "        node_adj = [[lookup[edge['id']], edge['weight']] for edge in adj]\n",
    "        yield from pack_intarr_2d(node_adj)\n",
    "        \n",
    "def deserialize_digraph(byteiter):\n",
    "    data = deserialize_adjd(byteiter)\n",
    "    return nx.readwrite.json_graph.adjacency_graph(data)\n",
    "\n",
    "def deserialize_adjd(byteiter):\n",
    "    byteiter = iter(byteiter)\n",
    "    nodes = unpack_strarr(byteiter)\n",
    "    \n",
    "    adjd = []\n",
    "    for node in nodes:\n",
    "        adj = unpack_intarr_2d(byteiter)\n",
    "        adjd.append([{'id': node, 'weight': weight} for id_, weight in adj])\n",
    "        \n",
    "    data = {\n",
    "        'directed': True, \n",
    "        'multigraph': False, \n",
    "        'nodes': [{'id': n} for n in nodes],\n",
    "        'adjacency': adjd,\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_fragment(it, fragment):\n",
    "    assert fragment == bytes(itertools.islice(it, len(fragment)))\n",
    "        \n",
    "def test_serialize_adjd():\n",
    "    nodes = [{'id': 'x'}, {'id': 'y'}]\n",
    "    adj = [[{'id': 'y', 'weight': 2}], [{'id': 'x', 'weight': 3}]]\n",
    "    g = {'nodes': nodes, 'adjacency': adj, 'multigraph': False, 'directed': True}\n",
    "    git = iter(serialize_adjd(g))\n",
    "\n",
    "    assert_fragment(git, b'\\x02\\x01x\\x01y')  # nodes\n",
    "    assert_fragment(git, b'\\x01\\x02\\x01\\x02')  # node x adjacency\n",
    "    assert_fragment(git, b'\\x01\\x02\\x00\\x03')  # node y adjacency\n",
    "    assert next(git, None) is None  # should be exhausted\n",
    "    \n",
    "def test_serialize_adjd2():\n",
    "    nodes = [{'id': 'x'}, {'id': 'y'}, {'id': 'zz'}]\n",
    "    adj = [\n",
    "        [{'id': 'y', 'weight': 2}, {'id': 'zz', 'weight': 0x44}], \n",
    "        [{'id': 'x', 'weight': 3}, {'id': 'zz', 'weight': 0x55}],\n",
    "    ]\n",
    "    g = {'nodes': nodes, 'adjacency': adj, 'multigraph': False, 'directed': True}\n",
    "    git = iter(serialize_adjd(g))\n",
    "#     gb = serialize_adjd(g)\n",
    "    assert_fragment(git, b'\\x03\\x01x\\x01y\\x02zz')  # nodes\n",
    "    assert_fragment(git, b'\\x02\\x02\\x01\\x02\\x02\\x44')  # node x adjacency\n",
    "    assert_fragment(git, b'\\x02\\x02\\x00\\x03\\x02\\x55')  # node y adjacency\n",
    "    assert next(git, None) is None  # should be exhausted\n",
    "    \n",
    "test_serialize_adjd()\n",
    "test_serialize_adjd2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "[[{'id': 'x', 'weight': 2}], [{'id': 'y', 'weight': 3}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-579-4bfff970cf44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# should be exhausted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtest_deserialize_adjd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtest_deserialize_adjd2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-579-4bfff970cf44>\u001b[0m in \u001b[0;36mtest_deserialize_adjd\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adjacency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adjacency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#     assert set(G.edges()) ==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: [[{'id': 'x', 'weight': 2}], [{'id': 'y', 'weight': 3}]]"
     ]
    }
   ],
   "source": [
    "def test_deserialize_adjd():\n",
    "    nodes = [{'id': 'x'}, {'id': 'y'}]\n",
    "    adj = [[{'id': 'y', 'weight': 2}], [{'id': 'x', 'weight': 3}]]\n",
    "    g = {'nodes': nodes, 'adjacency': adj, 'multigraph': False, 'directed': True}\n",
    "\n",
    "    bit = iter(\n",
    "        b'\\x02\\x01x\\x01y' +  # nodes\n",
    "        b'\\x01\\x02\\x01\\x02' +  # node x adjacency\n",
    "        b'\\x01\\x02\\x00\\x03')  # node y adjacency\n",
    "    \n",
    "    G = deserialize_adjd(bit) \n",
    "    assert next(bit, None) is None  # should be exhausted\n",
    "    \n",
    "    assert G['nodes'] == nodes\n",
    "    assert G['adjacency'] == adj, G['adjacency']\n",
    "#     assert set(G.edges()) == \n",
    "    \n",
    "def test_deserialize_adjd2():\n",
    "    nodes = [{'id': 'x'}, {'id': 'y'}, {'id': 'z'}]\n",
    "    adj = [\n",
    "        [{'id': 'y', 'weight': 2}, {'id': 'z', 'weight': 0x44}], \n",
    "        [{'id': 'x', 'weight': 3}, {'id': 'z', 'weight': 0x55}],\n",
    "    ]\n",
    "    g = {'nodes': nodes, 'adjacency': adj, 'multigraph': False, 'directed': True}\n",
    "\n",
    "    bit = iter(\n",
    "        b'\\x02\\x01x\\x01y' +  # nodes\n",
    "        b'\\x01\\x02\\x01\\x02' +  # node x adjacency\n",
    "        b'\\x01\\x02\\x00\\x03')  # node y adjacency\n",
    "    \n",
    "    print(deserialize_adjd(bit)) \n",
    "    assert next(bit, None) is None  # should be exhausted\n",
    "    \n",
    "test_deserialize_adjd()\n",
    "test_deserialize_adjd2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import importlib\n",
    "\n",
    "COMPRESSORS = {lib: None for lib in ['gzip', 'bz2', 'lzma']}\n",
    "for lib in COMPRESSORS:\n",
    "    with contextlib.suppress(ImportError):\n",
    "        COMPRESSORS[lib] = importlib.import_module(lib)\n",
    "        \n",
    "COMPRESSED_EXTS = {\n",
    "    'gz': 'gzip',\n",
    "    'bz2': 'bz2',\n",
    "    'xz': 'lzma',\n",
    "    'lzma': 'lzma',\n",
    "}\n",
    "def opener(fn, *args, **kwargs):\n",
    "    final_ext = fn.rsplit('.')[-1]\n",
    "    if final_ext in COMPRESSED_EXTS:\n",
    "        compressor = COMPRESSED_EXTS[final_ext]\n",
    "        compressor_lib = COMPRESSORS.get(compressor, None)\n",
    "        if compressor_lib is None:\n",
    "            raise RuntimeError('could not import library \"{}\"'.format(compressor))\n",
    "        return compressor_lib.open(fn, *args, **kwargs)\n",
    "    return open(fn, *args, **kwargs)\n",
    "\n",
    "def dump_word_digraph(G, filename):\n",
    "    dump = serialize_digraph(G)\n",
    "    with opener(filename, 'wb') as f:\n",
    "        f.write(dump)\n",
    "        \n",
    "def load_word_digraph(filename):\n",
    "    with opener(filename, 'rb') as f:\n",
    "        dump = f.read()\n",
    "    G = deserialize_digraph(dump)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uncompressed\n"
     ]
    }
   ],
   "source": [
    "dump_word_digraph(G, 'myfile.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gbz = load_word_digraph('myfile.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(G.edges()) == set(Gbz.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66633, 9335)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.edges()), len(Gbz.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit = iter(\n",
    "    b'\\x02\\x01x\\x01y' +  # nodes\n",
    "    b'\\x01\\x02\\x01\\x02' +  # node x adjacency\n",
    "    b'\\x01\\x02\\x00\\x03')  # node y adjacency\n",
    "tg = deserialize_digraph(bit)\n",
    "len(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "sGgz = gzip.compress(sG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "863315"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dracula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177123"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sGgz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270012"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sG = serialize_digraph(G)\n",
    "len(sG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9334"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"\\xc8v\\x08Jonathan\\x08Harker's\\x07Journal\\x02in\\tshorthand\\x01.\\x03may\\x08Bistritz\\x04left\\x06Munich\\x02at\\x01:\\x01P\\x01m\\x01,\\x02on\\x08arriving\\x06Vienna\\x05e\""
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sG[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpacked 9334 nodes\n"
     ]
    }
   ],
   "source": [
    "Gx = deserialize_digraph(iter(sG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Jonathan'},\n",
       " {'id': \"Harker's\"},\n",
       " {'id': 'Journal'},\n",
       " {'id': 'in'},\n",
       " {'id': 'shorthand'},\n",
       " {'id': '.'},\n",
       " {'id': 'may'},\n",
       " {'id': 'Bistritz'},\n",
       " {'id': 'left'},\n",
       " {'id': 'Munich'}]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjd['nodes'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'id': \"Harker's\", 'weight': 8},\n",
       "  {'id': 'Harker', 'weight': 10},\n",
       "  {'id': 'Nay', 'weight': 1},\n",
       "  {'id': ',', 'weight': 37},\n",
       "  {'id': 'some', 'weight': 1},\n",
       "  {'id': 'from', 'weight': 1},\n",
       "  {'id': 'and', 'weight': 9},\n",
       "  {'id': '.', 'weight': 14},\n",
       "  {'id': 'for', 'weight': 2},\n",
       "  {'id': 'is', 'weight': 6},\n",
       "  {'id': ';', 'weight': 3},\n",
       "  {'id': 'since', 'weight': 1},\n",
       "  {'id': 'was', 'weight': 6},\n",
       "  {'id': 'been', 'weight': 1},\n",
       "  {'id': '!', 'weight': 2},\n",
       "  {'id': 'awakes', 'weight': 1},\n",
       "  {'id': 'woke', 'weight': 2},\n",
       "  {'id': 'wants', 'weight': 1},\n",
       "  {'id': 'asks', 'weight': 1},\n",
       "  {'id': 'feels', 'weight': 1},\n",
       "  {'id': 'tries', 'weight': 1},\n",
       "  {'id': 'will', 'weight': 3},\n",
       "  {'id': 'sleeping', 'weight': 1},\n",
       "  {'id': 'away', 'weight': 1},\n",
       "  {'id': 'a', 'weight': 2},\n",
       "  {'id': 'with', 'weight': 2},\n",
       "  {'id': 'thought', 'weight': 1},\n",
       "  {'id': 'clutch', 'weight': 1},\n",
       "  {'id': 'kept', 'weight': 2},\n",
       "  {'id': 'why', 'weight': 1},\n",
       "  {'id': 'still', 'weight': 1},\n",
       "  {'id': 'rising', 'weight': 1},\n",
       "  {'id': 'may', 'weight': 2},\n",
       "  {'id': 'quite', 'weight': 1},\n",
       "  {'id': 'went', 'weight': 2},\n",
       "  {'id': 'would', 'weight': 3},\n",
       "  {'id': 'in', 'weight': 2},\n",
       "  {'id': 'at', 'weight': 1},\n",
       "  {'id': 'to', 'weight': 2},\n",
       "  {'id': 'put', 'weight': 1},\n",
       "  {'id': 'sat', 'weight': 1},\n",
       "  {'id': 'had', 'weight': 3},\n",
       "  {'id': 'held', 'weight': 1},\n",
       "  {'id': 'observe', 'weight': 1},\n",
       "  {'id': 'when', 'weight': 1},\n",
       "  {'id': 'saw', 'weight': 2},\n",
       "  {'id': 'that', 'weight': 1},\n",
       "  {'id': 'have', 'weight': 4},\n",
       "  {'id': 'came', 'weight': 1},\n",
       "  {'id': 'who', 'weight': 1},\n",
       "  {'id': 'coming', 'weight': 2},\n",
       "  {'id': ':', 'weight': 1},\n",
       "  {'id': 'dear', 'weight': 1},\n",
       "  {'id': 'leaving', 'weight': 1},\n",
       "  {'id': 'does', 'weight': 1},\n",
       "  {'id': 'took', 'weight': 1},\n",
       "  {'id': 'as', 'weight': 1},\n",
       "  {'id': 'looked', 'weight': 1},\n",
       "  {'id': 'go', 'weight': 1},\n",
       "  {'id': 'interrupted', 'weight': 1},\n",
       "  {'id': 'travelled', 'weight': 1},\n",
       "  {'id': 'tell', 'weight': 1},\n",
       "  {'id': 'said', 'weight': 1},\n",
       "  {'id': 'I', 'weight': 2},\n",
       "  {'id': 'dashed', 'weight': 1},\n",
       "  {'id': 'on', 'weight': 1},\n",
       "  {'id': 'knelt', 'weight': 1}],\n",
       " [{'id': 'Journal', 'weight': 31},\n",
       "  {'id': 'diary', 'weight': 2},\n",
       "  {'id': 'September', 'weight': 1},\n",
       "  {'id': 'presence', 'weight': 1},\n",
       "  {'id': 'suggestion', 'weight': 1},\n",
       "  {'id': 'hands', 'weight': 1},\n",
       "  {'id': 'quick', 'weight': 1},\n",
       "  {'id': 'head', 'weight': 1},\n",
       "  {'id': 'voice', 'weight': 1},\n",
       "  {'id': 'telegram', 'weight': 1},\n",
       "  {'id': 'arm', 'weight': 1},\n",
       "  {'id': 'forehead', 'weight': 1},\n",
       "  {'id': 'tongue', 'weight': 1},\n",
       "  {'id': 'silences', 'weight': 1},\n",
       "  {'id': 'grew', 'weight': 1},\n",
       "  {'id': 'coming', 'weight': 1},\n",
       "  {'id': 'heart', 'weight': 1},\n",
       "  {'id': 'hypnotic', 'weight': 2},\n",
       "  {'id': 'hand', 'weight': 1},\n",
       "  {'id': 'pale', 'weight': 1},\n",
       "  {'id': 'memorandum', 'weight': 1}],\n",
       " [{'id': 'in', 'weight': 1},\n",
       "  {'id': 'may', 'weight': 1},\n",
       "  {'id': 'when', 'weight': 2},\n",
       "  {'id': 'I', 'weight': 1},\n",
       "  {'id': 'of', 'weight': 1},\n",
       "  {'id': 'which', 'weight': 1},\n",
       "  {'id': 'July', 'weight': 1},\n",
       "  {'id': '.', 'weight': 26},\n",
       "  {'id': 'day', 'weight': 1},\n",
       "  {'id': ',', 'weight': 2},\n",
       "  {'id': 'gets', 'weight': 1},\n",
       "  {'id': 'yesterday', 'weight': 1},\n",
       "  {'id': 'unless', 'weight': 1},\n",
       "  {'id': 'first', 'weight': 1},\n",
       "  {'id': 'is', 'weight': 1},\n",
       "  {'id': 'be', 'weight': 1},\n",
       "  {'id': 'October', 'weight': 3},\n",
       "  {'id': 'as', 'weight': 1},\n",
       "  {'id': 'at', 'weight': 1},\n",
       "  {'id': 'continued', 'weight': 1},\n",
       "  {'id': 'November', 'weight': 1}]]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjd['adjacency'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': 0, 'links': 66615, 'nodes': 9336}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: len(nld[k]) for k in nld}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3648949"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 1, 'target': 2, 'weight': 3},\n",
       " {'source': 'Jonathan', 'target': \"Harker's\", 'weight': 8},\n",
       " {'source': 'Jonathan', 'target': 'Harker', 'weight': 10},\n",
       " {'source': 'Jonathan', 'target': 'Nay', 'weight': 1},\n",
       " {'source': 'Jonathan', 'target': ',', 'weight': 37},\n",
       " {'source': 'Jonathan', 'target': 'some', 'weight': 1},\n",
       " {'source': 'Jonathan', 'target': 'from', 'weight': 1},\n",
       " {'source': 'Jonathan', 'target': 'and', 'weight': 9},\n",
       " {'source': 'Jonathan', 'target': '.', 'weight': 14},\n",
       " {'source': 'Jonathan', 'target': 'for', 'weight': 2}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nld['links'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
